# -*- coding: utf-8 -*-
"""EmailSPAM-NaiveBayes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gt0h0dyeik7uqnhWmqva-25If7JSvUJD
"""

# import packages

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# import data
spam_df = pd.read_csv('spam.csv', encoding='latin-1')
spam_df

#spam_df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)
spam_df.rename(columns={'v1': 'Category', 'v2': 'Message'}, inplace=True)
spam_df

spam_df.groupby('Category').describe()

# turn ham/spam into numerical data, create a new column 'spam' with value 0/1
spam_df['spam'] = spam_df['Category'].apply(lambda x: 1 if x=='spam' else 0)
spam_df

# create train-test split

X_train, X_test, y_train, y_test = train_test_split(spam_df.Message, spam_df.spam, test_size=0.20)

X_train.describe()

#find word count and store data as a matrix

cv = CountVectorizer()
X_train_count = cv.fit_transform(X_train.values)
X_test_count = cv.transform(X_test)

X_train_count

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize CountVectorizer
cv = CountVectorizer()

# Assume X_train, X_test, y_train, y_test are already defined and preprocessed
X_train_count = cv.fit_transform(X_train.values)
X_test_count = cv.transform(X_test)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "MultinomialNB": MultinomialNB()
}

# Train and evaluate models
metrics = []

for name, model in models.items():
    model.fit(X_train_count, y_train)  # Train the model
    y_pred = model.predict(X_test_count)  # Predict on test data
    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy
    precision = precision_score(y_test, y_pred)  # Calculate precision
    recall = recall_score(y_test, y_pred)  # Calculate recall
    f1 = f1_score(y_test, y_pred)  # Calculate F1 score

    # Append metrics to list
    metrics.append({
        "Model": name,
        "Accuracy": f"{accuracy:.3f}",
        "Precision": f"{precision:.3f}",
        "Recall": f"{recall:.3f}",
        "F1 Score": f"{f1:.3f}"
    })

# Create a DataFrame to display metrics in table format
metrics_df = pd.DataFrame(metrics)

# Display the metrics table
metrics_df

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# Example: Assume your model and vectorizer are already loaded
# nb_model = your_trained_model
# cv = your_count_vectorizer

# Function to predict if the email is spam or not
def predict_spam(email_text):
    email_count = cv.transform([email_text])  # Transform the input text using the vectorizer
    prediction = nb_model.predict(email_count)  # Make the prediction using the model
    if prediction == 1:
        return "SPAM", "red"
    else:
        return "NOT SPAM", "green"

# Ask the user to input an email text
email_text = input("Enter the email text: ")

# Make the prediction
result, color = predict_spam(email_text)

# Output the result
print(f"\033[1;31m{result}\033[0m" if color == "red" else f"\033[1;32m{result}\033[0m")

email_text = input("Enter the email text: ")

# Make the prediction
result, color = predict_spam(email_text)

# Output the result
print(f"\033[1;31m{result}\033[0m" if color == "red" else f"\033[1;32m{result}\033[0m")